{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyJ25uz0kSaw"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Assignment 5 on Natural Language Processing\n",
    "\n",
    "## Date : 3rd Nov, 2020\n",
    "\n",
    "### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ao1nhg9RknmF"
   },
   "source": [
    "The central idea of this assignment is to explore various language models specifically LSTM based and transformer. We will explore how the size of the model effects the sequence generated. We will see both character based and word based models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONM5Q4SCe9Mr"
   },
   "source": [
    "Please submit with outputs. Submissions without predicted outputs will be penalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXdkhxZAXnTW"
   },
   "source": [
    "# Word Based LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbU5DRolXseI"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import numpy\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2NR3RFFYOT8"
   },
   "source": [
    "Do basic pre processing which includes lowering etc\n",
    "Check the dataset and apply suitable preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQvfF2NjXxGj"
   },
   "outputs": [],
   "source": [
    "# Load the data and preprocess data and store corpus in raw_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eug68GOecM8Z"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters of the model\n",
    "vocab_size = 3000 # choose based on statistics\n",
    "oov_tok = '<OOV>'\n",
    "embedding_dim = 100\n",
    "padding_type='post'\n",
    "trunc_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWNBOlJ5cQym"
   },
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts([raw_text])\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNRJDbFcdHbO"
   },
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "tokens = tokenizer.texts_to_sequences([raw_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykfI4FrwdyJe"
   },
   "outputs": [],
   "source": [
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(tokens) - seq_length-1 , 1):\n",
    "  seq_in = tokens[i:i + seq_length]\n",
    "  seq_out = tokens[i + seq_length]\n",
    "\n",
    "  if seq_out==1: #Skip samples where target word is OOV\n",
    "    continue\n",
    "    \n",
    "  dataX.append(seq_in)\n",
    "  dataY.append(seq_out)\n",
    " \n",
    "N = len(dataX)\n",
    "print (\"Total training data size: \", N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJmGr1xId8cO"
   },
   "outputs": [],
   "source": [
    "X = numpy.array(dataX)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = numpy.array(dataY)\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QPApRA-d9JV"
   },
   "outputs": [],
   "source": [
    "# with embedding\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=seq_length),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
    "    keras.layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14ClAAYpeCVO"
   },
   "outputs": [],
   "source": [
    "# Use validation split of 0.2 while training\n",
    "model.fit(X, y, epochs=#, batch_size=128, # ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mg9WSEwYeMAH"
   },
   "outputs": [],
   "source": [
    "reverse_word_map = ## Create word to idx map using tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_mhL0J0eQku"
   },
   "outputs": [],
   "source": [
    "# Complete the code to return next n words greedily\n",
    "def next_tokens(input_str, n): \n",
    "\tprint (\"Seed: \\n\",  input_str)\n",
    "\tfor i in range(n):\n",
    "\t\tprediction = model.predict( ## , verbose=0)\n",
    "\t\t# get next word index. Use reverse_word_map to get the word\n",
    "\t\treturn final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZ_NTQezeWYO"
   },
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "\n",
    "# start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "input_str = ' '.join([reverse_word_map[value] for value in pattern])\n",
    "\n",
    "print(next_tokens( input_str , 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-x8W-OSVgNi"
   },
   "outputs": [],
   "source": [
    "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
    " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
    " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
    " him or his sheep.\"\n",
    "\n",
    "# Use first 50 tokens from given input_str as input.(Use tokenizer to split to take first 50)\n",
    "print(next_tokens( # , 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5_R5Tngo3_D"
   },
   "source": [
    "# Character based LSTM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zZaHsejo57p"
   },
   "outputs": [],
   "source": [
    "# User the preprocess data and create raw_text\n",
    "\n",
    "# create mapping of unique characters to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "\n",
    "char_to_int = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkVVDbump0Wg"
   },
   "outputs": [],
   "source": [
    "# Print the total characters and character vacob size\n",
    "n_chars = \n",
    "n_vocab = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aVserymqE1b"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Prepare dataset where the input is sequence of 100 characters and target is next character.\n",
    "'''\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    # Write code here\n",
    "\n",
    "\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ic4yf4hNqc7T"
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.array(dataX)\n",
    "\n",
    "# one hot encode the output variable\n",
    "dataY = numpy.array(dataY)\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvawnjFVqhMi"
   },
   "outputs": [],
   "source": [
    "embedding_dim =100\n",
    "max_length =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ek5DqNeTqkAZ"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab, embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFoStJIOqpM6"
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekXYhzeUrGaq"
   },
   "outputs": [],
   "source": [
    "#implement mapping of integer to character\n",
    "int_to_char ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OixPrw6vq15j"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Complete code below to get the generated string using the model.\n",
    "'''\n",
    "def predict_next_100_chars(pattern,x):\n",
    "\tfor i in range(x):\n",
    "\t\t# Complete Code\n",
    "\t\tprediction = model.predict(#, verbose=0)\n",
    "\n",
    "\n",
    "\treturn final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHH_I5QiUxnY"
   },
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "input_str = ''.join([int_to_char[value] for value in pattern])\n",
    "\n",
    "print(predict_next_100_chars(input_str,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iutpuJAgrgU8"
   },
   "outputs": [],
   "source": [
    "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
    " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
    " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
    " him or his sheep.\"\n",
    "\n",
    " # Use first 100 characeters from given input_str as input and generate next 200 characters.\n",
    "\n",
    " \n",
    "print(predict_next_100_chars(input_str,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8shbcukr0tJ"
   },
   "source": [
    "## Character based LSTM Model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWBPCrTdr46U"
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(n_vocab, embedding_dim, input_length=max_length))\n",
    "model1.add(LSTM(256, input_shape=(X.shape[1], embedding_dim),return_sequences=True))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(LSTM(256))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(y.shape[1], activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZxrtjFIr63L"
   },
   "outputs": [],
   "source": [
    "model1.fit(X, y, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6RTfHN6r8wh"
   },
   "outputs": [],
   "source": [
    "# Generate the sequence similar to above methods\n",
    "\n",
    "'''\n",
    "Complete code below to get the generated string using the model.\n",
    "'''\n",
    "def predict_next_100_chars(pattern,x):\n",
    "\tfor i in range(x):\n",
    "\t\t# Complete Code\n",
    "\t\tprediction = model1.predict(#, verbose=0)\n",
    "\n",
    "\n",
    "\treturn final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CKJXZ4tYmL0"
   },
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "input_str = ''.join([int_to_char[value] for value in pattern])\n",
    "\n",
    "print(predict_next_100_chars(input_str,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyRCfLe5YmL7"
   },
   "outputs": [],
   "source": [
    "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
    " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
    " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
    " him or his sheep.\"\n",
    "\n",
    " # Use first 100 characeters from given input_str as input and generate next 200 characters.\n",
    "\n",
    " \n",
    "print(predict_next_100_chars(input_str,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPrjxjoNsaQC"
   },
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW5_IlBeZAPq"
   },
   "source": [
    "**Question:** What are your observations based on the model(all) outputs on train data(in domain) vs unseen data(out of domain) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YryzP1upZv5h"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBzCD0I0Z3uP"
   },
   "source": [
    "**Question:** What did you observe in the outputs of char LSTM model1 vs char LSTM model2 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUHxHmXZaNdn"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rShXR9sYsc4D"
   },
   "source": [
    "# Bonus (Not to be graded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2N-l6VSshhC"
   },
   "source": [
    "## Transformer based language model (Bert)\n",
    "You can run the below cells to predict the next word using pretrained BERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67c13Yx9sgiW"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gF2nncdsqcx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import string\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tppcQN298Qs"
   },
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "  try:\n",
    "    if model_name.lower() == \"bert\":\n",
    "      bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "      bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased').eval()\n",
    "      return bert_tokenizer,bert_model\n",
    "  except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOcQKOGG-H8F"
   },
   "outputs": [],
   "source": [
    "def decode(tokenizer, pred_idx, top_clean):\n",
    "  ignore_tokens = string.punctuation + '[PAD]'\n",
    "  tokens = []\n",
    "  for w in pred_idx:\n",
    "    token = ''.join(tokenizer.decode(w).split())\n",
    "    if token not in ignore_tokens:\n",
    "      tokens.append(token.replace('##', ''))\n",
    "  return '\\n'.join(tokens[:top_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQkjSmGv-TxO"
   },
   "outputs": [],
   "source": [
    "def encode(tokenizer, text_sentence, add_special_tokens=True):\n",
    "  text_sentence = text_sentence.replace('<mask>', tokenizer.mask_token)\n",
    "  # if <mask> is the last token, append a \".\" so that models dont predict punctuation.\n",
    "  if tokenizer.mask_token == text_sentence.split()[-1]:\n",
    "    text_sentence += ' .'\n",
    "  input_ids = torch.tensor([tokenizer.encode(text_sentence, add_special_tokens=add_special_tokens)])\n",
    "  mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "  return input_ids, mask_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFWOHB5n-ZQh"
   },
   "outputs": [],
   "source": [
    "def get_all_predictions(text_sentence, top_clean=5):\n",
    "  input_ids, mask_idx = encode(bert_tokenizer, text_sentence)\n",
    "  with torch.no_grad():\n",
    "    predict = bert_model(input_ids)[0]\n",
    "    print(predict.shape)\n",
    "    \n",
    "    bert = decode(bert_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n",
    "  return {'bert': bert}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lQHVc0l-igk"
   },
   "outputs": [],
   "source": [
    "def get_prediction_eos(input_text):\n",
    "  try:\n",
    "    input_text += ' <mask>'\n",
    "    res = get_all_predictions(input_text, top_clean=int(top_k))\n",
    "    return res\n",
    "  except Exception as error:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQZ-swMD9lyU"
   },
   "outputs": [],
   "source": [
    "# Below code predicts the next top_k words. You can modify this to get next n words using top_k=1 and greedy decoding it. \n",
    "top_k= 3\n",
    "print('Predict next ', top_k, ' words')\n",
    "model_name = 'BERT'\n",
    "bert_tokenizer, bert_model  = load_model(model_name) \n",
    "input_text = \"\" ### GIVE YOUR INPUT STRING HERE\n",
    "res = get_prediction_eos(input_text)\n",
    "answer = []\n",
    "print(res['bert'].split(\"\\n\"))\n",
    "for i in res['bert'].split(\"\\n\"):\n",
    "  answer.append(i)\n",
    "  answer_as_string = \"    \".join(answer)\n",
    "\n",
    "print(answer_as_string)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment-5 NLP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
